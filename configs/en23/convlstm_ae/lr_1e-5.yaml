Architecture: "convlstm_ae"

Seed: 42

# Dataset setting
Setting: "en23"

# Logger for Tensorboard
Logger:
    save_dir: "experiments/"

Checkpointer:
    save_top_k: 1
    save_last: True
    every_n_epochs: 1

Trainer:
    gpus: 1
    strategy: 'ddp_find_unused_parameters_false'
    #deterministic: True
    log_every_n_steps: 32
    profiler: 'simple'
    accumulate_grad_batches: 8
    #fast_dev_run: True
    #log_gpu_memory: 'all'
    #weights_summary: 'full'
    max_epochs: 50
    #limit_train_batches: 32
    #limit_val_batches: 32
    gradient_clip_val: 1
    #val_check_interval: 0.25
  
Data:
    base_dir: "/workspace/data/s3/earthnet/earthnet2023/" #"/scratch/crobin/earthnet2023/" #
    test_track: "iid"
    target: "ndvi"
    # With a batch of size N, a DPP strategy with D devices, and accumulate_grad_batches = K, the effective batch size = K*D*N. 
    # With a DP strategy, the effective batch size= K*N.
    train_batch_size: 4
    val_batch_size: 4
    test_batch_size: 4
    num_workers: 4 # Common bottleneck during the training, increase until saturation. 

Task:
    loss:
        name: "MaskedL2NDVILoss"  # loss computed with the non-vegetation pixels masked.
        min_lc: 40
        max_lc: 90
        ndvi_pred_idx: 0
        ndvi_targ_idx: 0
        pred_mask_value: -1
        scale_by_std: False
    context_length: 20
    target_length: 10
    n_stochastic_preds: 1
    optimization:
        optimizer:
            - 
                name: AdamW
                args: 
                    betas: [0.9, 0.999]
                lr_per_sample: 0.000001
        lr_shedule:
            -
                name:  ExponentialLR  # ReduceLROnPlateau factor: 0.1, patience: 3
                args:
                    gamma: 0.99
    n_log_batches: 2
    compute_metric_on_test: True

Model:
    hidden_dim: [64, 64, 64, 64]
    kernel_size: 3
    bias: True
    skip_connections: True
    num_inputs: 49
    num_outputs: 8