
Setting: "en22"

Logger:
    save_dir: "experiments/"

Checkpointer:
    save_top_k: 1
    save_last: True
    every_n_epochs: 5

Trainer:
    gpus: [0, 1]
    strategy: 'ddp'
    accumulate_grad_batches: 8
    precision: 16
    log_every_n_steps: 32
    max_epochs: 50
    gradient_clip_val: 1
    val_check_interval: 0.1
  
Data:
    base_dir: "/workspace/data/scratch/L2_minicubes/" 
    test_track: "iid"
    train_batch_size: 4 #96
    val_batch_size: 1
    test_batch_size: 1 #96
    num_workers: 8 #2

Task:
    loss:
        name: "masked"
        args: {
            distance_type: "L2",
            rescale: True
        }
        dist_scale: 1
        state_shedule: {
            initial_weight: 0,
            increase_from_step: 1500,
            weight_step: 0.00001,
            max_weight: 1
        }
        inference_shedule: {
            initial_weight: 0,
            increase_from_step: 3000,
            weight_step: 0.0000001,
            max_weight: 1
        }
        residuals_shedule: {
            initial_weight: 0,
            increase_from_step: 1500,
            weight_step: 0.00001,
            max_weight: 1
        }
        ndvi: True
        min_lc: 2
        max_lc: 6
        comp_ndvi: True
    context_length: 9
    target_length: 36
    n_stochastic_preds: 1
    optimization:
        optimizer:
            - 
                name: Adam
                args: 
                    betas: [0.9, 0.999]
                lr_per_sample: 0.000001
        lr_shedule:
            -
                name: MultiStepLR
                args:
                    milestones: [150] #[2, 20, 50, 90]
                    gamma: 0.1
    n_log_batches: 8
    train_batch_size: 4 #96
    val_batch_size: 1
    test_batch_size: 1 #96
    compute_metric_on_test: True

Model:
    context_length: 9
    target_length: 36
    hidden_dim: [64, 64, 64, 64]
    kernel_size: 3
    num_layers: 3
    bias: True
    return_all_layers: False
    lc_min: 2
    lc_max: 6
    input: 'NDVI+T+W'
    skip_connections: True